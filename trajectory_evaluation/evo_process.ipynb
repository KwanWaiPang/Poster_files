{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The quantitative accuracy evaluation of VO/VIO/SLAM\n",
    "First, the estimated trajectory needs to be aligned with the groundtruth.\n",
    "Then, the trajectory estimation error can be calculated from the aligned estimate and the groundtruth using certain error metrics (i.e., absolute trajectory error and relative error).\n",
    "\n",
    "In this file, we demonstrate how to calculate these evaluation metrics and visualize the results using [evo](https://github.com/MichaelGrupp/evo) in python code.\n",
    "* More details of command line interface can be seen in [Our Github Repository](https://github.com/arclab-hku/Event_based_VO-VIO-SLAM/issues/5)\n",
    "* While more details of the quantitative evaluation of pose tracking can be seen in [our paper](), and [rpg_trajectory_evaluation](https://www.zora.uzh.ch/id/eprint/175991/1/IROS18_Zhang.pdf).\n",
    "* You need to install the evo package before running the code.\n",
    "~~~\n",
    " pip install evo\n",
    "~~~\n",
    "\n",
    "## Tips on using this ipynb\n",
    "* Setup the conda environment following [Link](https://github.com/KwanWaiPang/ESIM_comment), and select the kernel as vid2e; When using the Server, you can run the following command to speed up the process:\n",
    "~~~\n",
    "conda activate vid2e\n",
    "pip install ipykernel\n",
    "~~~\n",
    "* Download the sampled dataset\n",
    "~~~\n",
    "conda activate nerf-ngp\n",
    "bypy list\n",
    "bypy download [remotepath] [localpath]\n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import ultils to process the rosbag\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import multiprocessing #多线程处理\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt #绘图\n",
    "\n",
    "\n",
    "import rosbag #处理rosbag\n",
    "from cv_bridge import CvBridge\n",
    "\n",
    "import sys\n",
    "sys.path.append('/home/gwp/Poster_files/trajectory_evaluation')  # add the path of the utils, please note that when you change the code, the kernel should be restarted\n",
    "from utils.bag_utils import read_H_W_from_bag, read_tss_us_from_rosbag, read_images_from_rosbag, read_evs_from_rosbag, read_calib_from_bag, read_t0us_evs_from_rosbag, read_poses_from_rosbag\n",
    "print(\"import ultils to process the rosbag\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all the topic name in this rosbag ['/cpy_uav/viconros/odometry', '/pose_graph/evio_odometry']\n"
     ]
    }
   ],
   "source": [
    "rosbag_file=\"../dataset/ESVIO_hku_agg_small_flip.bag\"\n",
    "# read data from rosbag\n",
    "bag_data = rosbag.Bag(rosbag_file, \"r\")\n",
    "topics = list(bag_data.get_type_and_topic_info()[1].keys())\n",
    "print(\"all the topic name in this rosbag\",topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2945/2945 [00:00<00:00, 12438.32it/s]\n",
      "100%|██████████| 59060/59060 [00:03<00:00, 19436.99it/s]\n"
     ]
    }
   ],
   "source": [
    "gt_topic_name='/cpy_uav/viconros/odometry';\n",
    "est_topic_name='/pose_graph/evio_odometry';\n",
    "gt_poses, tss_gt_us =read_poses_from_rosbag(bag_data,gt_topic_name)\n",
    "assert np.all(tss_gt_us == sorted(tss_gt_us)) \n",
    "est_poses, tss_est_us =read_poses_from_rosbag(bag_data,est_topic_name)\n",
    "assert np.all(tss_est_us == sorted(tss_est_us)) #assert that the timestamps are sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define several useful functions\n",
    "import evo\n",
    "import evo.main_ape as main_ape\n",
    "from evo.core import sync, metrics\n",
    "from evo.core.trajectory import PoseTrajectory3D\n",
    "from evo.core.metrics import PoseRelation\n",
    "\n",
    "def ate(traj_ref, traj_est, timestamps):\n",
    "    traj_est = PoseTrajectory3D(\n",
    "        positions_xyz=traj_est[:,:3],\n",
    "        orientations_quat_wxyz=traj_est[:,3:], # TODO EVO uses wxyz, we use xyzw\n",
    "        timestamps=timestamps)\n",
    "\n",
    "    traj_ref = PoseTrajectory3D(\n",
    "        positions_xyz=traj_ref[:,:3],\n",
    "        orientations_quat_wxyz=traj_ref[:,3:],  # TODO EVO uses wxyz, we use xyzw\n",
    "        timestamps=timestamps)\n",
    "    \n",
    "    result = main_ape.ape(traj_ref, traj_est, est_name='traj', \n",
    "        pose_relation=PoseRelation.translation_part, align=True, correct_scale=True)\n",
    "\n",
    "    return result.stats[\"rmse\"]\n",
    "\n",
    "def ate_real(traj_ref, tss_ref_us, traj_est, tstamps):\n",
    "\n",
    "        # obtain the data structure of pose+timestamp\n",
    "        # please refer to:https://github.com/MichaelGrupp/evo/blob/63ea6f087bf6aca9e9feef193c605b0489cca4cd/evo/core/trajectory.py#L372\n",
    "        evoGT = PoseTrajectory3D(\n",
    "                positions_xyz=traj_ref[:,:3],\n",
    "                orientations_quat_wxyz=traj_ref[:,3:], # EVO uses wxyz\n",
    "                timestamps=(np.array(tss_ref_us)/1e6))\n",
    "        \n",
    "        # get the ground truth trajectory length\n",
    "        gtlentraj = evoGT.get_infos()[\"path length (m)\"]\n",
    "\n",
    "        evoEst = PoseTrajectory3D(\n",
    "                positions_xyz=traj_est[:,:3],#translation part\n",
    "                orientations_quat_wxyz=traj_est[:,3:], # EVO uses wxyz\n",
    "                timestamps=np.array(tstamps)/1e6)#change the unit from us to s\n",
    "        \n",
    "        # if the size of the two trajectories are the same, we can directly calculate the ATE\n",
    "        # if traj_ref.shape == traj_est.shape:\n",
    "        #         assert np.all(tss_ref_us == tstamps)\n",
    "        #         return ate(traj_ref, traj_est, tstamps)*100, evoGT, evoEst\n",
    "        \n",
    "        # The metrics require the trajectories to be associated via matching timestamps:\n",
    "        # please refer to:https://github.com/MichaelGrupp/evo/blob/63ea6f087bf6aca9e9feef193c605b0489cca4cd/evo/core/sync.py#L67\n",
    "        evoGT, evoEst = sync.associate_trajectories(evoGT, evoEst, max_diff=1)\n",
    "        # （absolute pose error ）\n",
    "        ape_trans = main_ape.ape(evoGT, evoEst, pose_relation=metrics.PoseRelation.translation_part, align=True, correct_scale=True)\n",
    "        # get the rmse value，and then multiply 100 to get the cm\n",
    "        evoATE = ape_trans.stats[\"rmse\"]\n",
    "        MPE = ape_trans.stats[\"mean\"] / gtlentraj * 100 # mean pose error of our EVIO\n",
    "        return evoATE, MPE, evoGT, evoEst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ATE[m]: 0.169 | MPE[%/m]: 0.351\n"
     ]
    }
   ],
   "source": [
    "ate_score, MPE, evoGT, evoEst = ate_real(gt_poses, tss_gt_us, est_poses, tss_est_us)\n",
    "res_str = f\"\\nATE[m]: {ate_score:.03f} | MPE[%/m]: {MPE:.03f}\"\n",
    "print(res_str)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vid2e",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
